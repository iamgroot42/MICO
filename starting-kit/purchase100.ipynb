{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membership Inference Competition (MICO) @ IEEE SatML 2023: Purchase-100\n",
    "\n",
    "Welcome to the MICO competition!\n",
    "\n",
    "This notebook will walk you through the process of creating and packaging a submission to one of the challenges.\n",
    "\n",
    "Let's start by downloading and extracting the archive for the Purchase-100 challenge.\n",
    "\n",
    "**NOTE**: Public anonymous access to the competition data is disabled. \n",
    "Upon registering for the competition, you will be shown a URL with an embedded bearer token that you must use instead of the URL below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "The archive was extracted under the `purchase100` folder containing 3 sub-folders, one for each of the scenarios in the challenge:\n",
    "\n",
    "- `purchase100_lo`  : Models trained with DP-SGD and a small privacy budget ($\\epsilon \\approx 4$) \n",
    "- `purchase100_hi`  : Models trained with DP-SGD and a large privacy budget ($\\epsilon \\approx 10$) \n",
    "- `purchase100_inf` : Models trained without differential privacy guarantee ($\\epsilon = \\infty$)\n",
    "\n",
    "Each of these folders contains 3 other folders:\n",
    "\n",
    "- `train`: Models with metadata allowing to reconstruct their full training datasets. Use these to develop your attacks without having to train your own models.\n",
    "- `dev`: Models with metadata allowing to reconstruct just the set of challenge examples. Membership predictions for these challenges will be used to evaluate submissions during the competition and update the live scoreboard in CodaLab. \n",
    "- `final`: Models with metadata allowing to reconstruct just the set of challenge examples. Membership predictions for these challenges will be used to evaluate submissions when the competition closes and to determine the final ranking.\n",
    "\n",
    "Each model folder in `train`, `dev`, and `final` contains a `model.pt` file with the model weights (a serialized PyTorch `state_dict`). There are 100 models in `train`, and 50 models in each of `dev` and `final`.\n",
    "\n",
    "Models in the `train` folder come with 3 PRNG seeds used to reconstruct the set of member and non-member challenge examples, and the rest of the examples in the training dataset of the model. Additionally (and redundantly), a `solution.csv` file reveals the membership information of the challenge examples.\n",
    "\n",
    "Models in the `dev` and `final` folders contain just 1 PRNG seed used to reconstruct the set of challenge examples, without revealing which were included in the training dataset.\n",
    "\n",
    "We provide utilities to reconstruct the different data splits from provided seeds and to load models as classes inheriting from `torch.nn.Module`. If you use TensorFlow, JAX, or any other framework, you can easily convert the models to the appropriate format (e.g. using ONXX).\n",
    "\n",
    "Here's a summary of how the contents are structured:\n",
    "\n",
    "- `purchase100_lo`\n",
    "  - `train`\n",
    "      - `model_0`\n",
    "        - `model.pt`: Serialized model weights\n",
    "        - `seed_challenge`: PRNG seed used to select a list of 100 challenge examples\n",
    "        - `seed_training`: PRNG seed used to select the non-challenge examples in the training dataset\n",
    "        - `seed_membership`: PRNG seed used to split the set of challenge examples into members and non-members (100 of each)\n",
    "        - `solution.csv`: Membership information of the challenge examples (`1` for member, `0` for non-member)\n",
    "      - ...\n",
    "  - `dev`\n",
    "      - `model_100`\n",
    "        - `model.pt`\n",
    "        - `seed_challenge`\n",
    "      - ...\n",
    "  - `final`\n",
    "    - `model_150`\n",
    "      - `model.pt`\n",
    "      - `seed_challenge`\n",
    "    - ...\n",
    "- `purchase100_hi`\n",
    "  - ...\n",
    "- `purchase100_inf`\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://membershipinference.blob.core.windows.net/mico/purchase100.zip?si=purchase100&spr=https&sv=2021-06-08&sr=b&sig=YzJUTPoNndtIy0y2666XnPXS4WBF%2BbN7kbVM2soQNoU%3D\"\n",
    "filename = \"purchase100.zip\"\n",
    "md5 = \"67eba1f88d112932fe722fef85fb95fd\"\n",
    "\n",
    "try:\n",
    "    download_and_extract_archive(url=url, download_root=os.curdir, extract_root=None, filename=filename, md5=md5, remove_finished=False)\n",
    "except urllib.error.HTTPError as e:\n",
    "    print(e)\n",
    "    print(\"Have you replaced the URL above with the one you got after registering?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "from torchvision.datasets.utils import download_and_extract_archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Your task as a competitor is to produce, for each model in `dev` and `final`, a CSV file listing your confidence scores (values between 0 and 1) for the membership of the challenge examples. You must save these scores in a `prediction.csv` file and place it in the same folder as the corresponding model. A submission to the challenge is an an archive containing just these `prediction.csv` files.\n",
    "\n",
    "**You must submit predictions for both `dev` and `final` when you submit to CodaLab.**\n",
    "\n",
    "In the following, we will show you how to compute predictions from a basic membership inference attack and package them as a submission archive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import csv\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from sklearn import metrics\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.distributions import normal\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from mico_competition import ChallengeDataset, load_purchase100, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_preds(preds):\n",
    "    # Normalize to unit interval\n",
    "    min_prediction = np.min(preds)\n",
    "    max_prediction = np.max(preds)\n",
    "    preds = (preds - min_prediction) / (max_prediction - min_prediction)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack based on checking robustness in neighborhood\n",
    "@torch.no_grad()\n",
    "def neighborhood_robustness(model, features, epsilon: float, n_neighbors: int):\n",
    "    batch_size_desired = len(features)\n",
    "    noise = normal.Normal(0, epsilon)\n",
    "    l2_diffs = []\n",
    "    base_preds = model(features).cpu().numpy()\n",
    "    n_classes = base_preds.shape[1]\n",
    "    for i, feature in enumerate(features):\n",
    "        neighbors = []\n",
    "        for _ in range(n_neighbors):\n",
    "            neighbors.append(feature + noise.sample(feature.shape).to(feature.device))\n",
    "        neighbors = torch.stack(neighbors, 0)\n",
    "        prediction = model(neighbors).cpu().numpy()\n",
    "        # Use L2 to check robustness\n",
    "        predictions_diff = np.linalg.norm(prediction - base_preds[i], axis=1)\n",
    "        l2_diffs.append(np.mean(predictions_diff))\n",
    "    return np.array(l2_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighborhood_and_loss(model, features, labels, as_features: bool = False):\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    features_collect = []\n",
    "    n_neighbors = 20\n",
    "\n",
    "    # Neighborhood robustness attack (near)\n",
    "    epsilon = 0.01\n",
    "    n_neighbors = 10\n",
    "    predictions_robust = neighborhood_robustness(model, features, epsilon, n_neighbors)\n",
    "    if as_features:\n",
    "        features_collect.append(predictions_robust)\n",
    "    predictions_robust = normalize_preds(predictions_robust)\n",
    "    predictions_robust = (1 - predictions_robust)\n",
    "    \n",
    "    # Neighborhood robustness attack (far)\n",
    "    epsilon = 0.1\n",
    "    n_neighbors = 10\n",
    "    predictions_robust = neighborhood_robustness(model, features, epsilon, n_neighbors)\n",
    "    if as_features:\n",
    "        features_collect.append(predictions_robust)\n",
    "    predictions_robust = normalize_preds(predictions_robust)\n",
    "    predictions_robust = (1 - predictions_robust)\n",
    "    \n",
    "    # Neighborhood robustness attack (further)\n",
    "    epsilon = 0.3\n",
    "    n_neighbors = 10\n",
    "    predictions_robust = neighborhood_robustness(model, features, epsilon, n_neighbors)\n",
    "    if as_features:\n",
    "        features_collect.append(predictions_robust)\n",
    "    predictions_robust = normalize_preds(predictions_robust)\n",
    "    predictions_robust = (1 - predictions_robust)\n",
    "            \n",
    "    # Loss Threshold Attack\n",
    "    output = model(features)\n",
    "    predictions = -criterion(output, labels).detach().numpy()\n",
    "    if as_features:\n",
    "        features_collect.append(predictions)\n",
    "        return np.array(features_collect).T\n",
    "    predictions = normalize_preds(predictions)\n",
    "            \n",
    "    predictions += predictions_robust\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_norm(model, features, labels):\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    features_collected = []\n",
    "    for feature, label in zip(features, labels):\n",
    "        model.zero_grad()\n",
    "        feature_var = Variable(feature)\n",
    "        output = model(feature)\n",
    "        loss = criterion(torch.unsqueeze(output, 0), torch.unsqueeze(label, 0))\n",
    "        loss.backward()\n",
    "        features_collected.append([torch.linalg.norm(x.grad.detach()).item() for x in model.parameters()])\n",
    "    features_collected = np.array(features_collected)\n",
    "    return features_collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_update_norms(model, features, labels):\n",
    "    lr = 0.001\n",
    "    n_steps = 10\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    features_collected = []\n",
    "    for feature, label in zip(features, labels):\n",
    "        features_inside = []\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "        for _ in range(n_steps):\n",
    "            optimizer.zero_grad()\n",
    "            feature_var = Variable(feature)\n",
    "            output = model(feature)\n",
    "            loss = criterion(torch.unsqueeze(output, 0), torch.unsqueeze(label, 0))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            features_inside.append([torch.linalg.norm(x.grad.detach()).item() for x in model.parameters()])\n",
    "        features_collected.append(np.array(features_inside))\n",
    "    features_collected = np.array(features_collected)\n",
    "    return features_collected.reshape(features_collected.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(model, features, labels):\n",
    "    layerwise_features = {}\n",
    "    # Define hook\n",
    "    def get_features(name):\n",
    "        def hook(model, input, output):\n",
    "            layerwise_features[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    preds = model(features)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_and_robustness(model, features, labels):\n",
    "    features_1 = neighborhood_and_loss(model, features, labels, as_features=True)\n",
    "    features_2 = get_gradient_norm(model, features, labels)\n",
    "    combined_feratures = np.concatenate((features_1, features_2), 1)\n",
    "    return combined_feratures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect \"training data\" using models from train split\n",
    "def collect_training_data():\n",
    "    CHALLENGE = \"purchase100\"\n",
    "    LEN_TRAINING = 150000\n",
    "    LEN_CHALLENGE = 100\n",
    "\n",
    "    scenarios = os.listdir(CHALLENGE)\n",
    "    phases = ['dev', 'final', 'train']\n",
    "\n",
    "    dataset = load_purchase100(dataset_dir=\"/u/as9rw/work/MICO/data\")\n",
    "\n",
    "    collected_features = {x:[] for x in scenarios}\n",
    "    collected_labels = {x:[] for x in scenarios}\n",
    "    phase = \"train\"\n",
    "    for scenario in tqdm(scenarios, desc=\"scenario\"):\n",
    "        root = os.path.join(CHALLENGE, scenario, phase)\n",
    "        for model_folder in tqdm(sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])), desc=\"model\"):\n",
    "            path = os.path.join(root, model_folder)\n",
    "            challenge_dataset = ChallengeDataset.from_path(path, dataset=dataset, len_training=LEN_TRAINING)\n",
    "            challenge_points = challenge_dataset.get_challenges()\n",
    "            \n",
    "            model = load_model('purchase100', path)\n",
    "            challenge_dataloader = torch.utils.data.DataLoader(challenge_points, batch_size=2*LEN_CHALLENGE)\n",
    "            features, labels = next(iter(challenge_dataloader))\n",
    "\n",
    "            # Based on gradients\n",
    "            # processed_features = get_gradient_norm(model, features, labels)\n",
    "\n",
    "            # Based on loss + robustness\n",
    "            # processed_features = neighborhood_and_loss(model, features, labels, as_features=True)\n",
    "\n",
    "            # Based on loss + robustness + gradients\n",
    "            # processed_features = gradient_and_robustness(model, features, labels)\n",
    "\n",
    "            # Based on neighborhood robustness\n",
    "            # processed_features = neighborhood_robustness(model, features, 0.1, 50)\n",
    "            \n",
    "            # Based on multiple gradient updates\n",
    "            processed_features = get_gradient_update_norms(model, features, labels)\n",
    "\n",
    "            # Collect features\n",
    "            collected_features[scenario].append(processed_features)\n",
    "            \n",
    "            # Get labels for membership\n",
    "            collected_labels[scenario].append(challenge_dataset.get_solutions())\n",
    "            np_y = np.array(challenge_dataset.get_solutions())\n",
    "            \n",
    "#             import matplotlib.pyplot as plt\n",
    "#             print(processed_features.shape[1])\n",
    "#             plt.scatter(processed_features[np_y == 0][0], processed_features[np_y == 0][1], color='r')\n",
    "#             plt.scatter(processed_features[np_y == 1][0], processed_features[np_y == 1][1], color='b')\n",
    "# #             plt.plot(np.arange(np.sum(np_y == 0)), np.sort(processed_features[np_y == 0]))\n",
    "# #             plt.plot(np.arange(np.sum(np_y == 1)), np.sort(processed_features[np_y == 1]))\n",
    "#             return None, None\n",
    "    \n",
    "    for sc in scenarios:\n",
    "        collected_features[sc] = np.concatenate(collected_features[sc], 0)\n",
    "        collected_labels[sc] = np.concatenate(collected_labels[sc], 0)\n",
    "\n",
    "    return collected_features, collected_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the Purchase-100 dataset consisting of 197324 records and 600 attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a0132997d64fc2bcb3edf2594451a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scenario:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0415e16ef34e98957b7b5e998d7a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a3e71604b94f1c9fc2b12b7b1faa16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ac51d1d55941ccb749f9240b0d10d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_for_meta, Y_for_meta = collect_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purchase100_inf | Train: 0.575\n",
      "purchase100_inf | Validation: 0.587\n",
      "purchase100_inf | Validation (AUC) : 0.621\n",
      "\n",
      "purchase100_hi | Train: 0.519\n",
      "purchase100_hi | Validation: 0.518\n",
      "purchase100_hi | Validation (AUC) : 0.524\n",
      "\n",
      "purchase100_lo | Train: 0.509\n",
      "purchase100_lo | Validation: 0.506\n",
      "purchase100_lo | Validation (AUC) : 0.529\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train different meta-classifiers per scenario\n",
    "CHALLENGE = \"purchase100\"\n",
    "scenarios = os.listdir(CHALLENGE)\n",
    "meta_clfs = {x: MLPClassifier(hidden_layer_sizes=(32, 16, 8, 4), max_iter=300) for x in X_for_meta.keys()}\n",
    "# meta_clfs = {x: RandomForestClassifier(max_depth=12) for x in X_for_meta.keys()}\n",
    "# meta_clfs = {x: RandomForestClassifier(max_depth=10) for x in X_for_meta.keys()}\n",
    "for sc in scenarios: \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_for_meta[sc], Y_for_meta[sc], test_size=0.1)\n",
    "    meta_clfs[sc].fit(X_train, y_train)\n",
    "    preds = meta_clfs[sc].predict_proba(X_test)[:, 1]\n",
    "    print(\"%s | Train: %.3f\" % (sc, meta_clfs[sc].score(X_train, y_train)))\n",
    "    print(\"%s | Validation: %.3f\" % (sc, meta_clfs[sc].score(X_test, y_test)))\n",
    "    print(\"%s | Validation (AUC) : %.3f\" % (sc, metrics.roc_auc_score(y_test, preds)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use given datapoint, train models w and w/o that point\n",
    "# Adapt KL test (from our SaTML paper) to make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another idea- perform one step of GD on model with datapoint, and compare gradient updates with\n",
    "# cases where member was not seen before, and use this as a feature for a meta-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plain old permutation-invariant network-based meta-classifier, but also take as input\n",
    "# The raw datapoint. Hope meta-classifier learns to form associations, but not sure how to\n",
    "# design such a meta-classifier (modifications). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_k_points(model, data, discard_percentage=):\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    dataloader = torch.utils.data.DataLoader(data, batch_size=1000, shuffle=False)\n",
    "    loss_vals = []\n",
    "    for x, y in dataloader:\n",
    "        loss_vals.append(criterion(model(x), y).detach())\n",
    "    loss_vals = ch.cat(loss_vals).cpu().numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_retraining_attack(model, x, challenge_dataset):\n",
    "    LEN_TRAINING = 150000\n",
    "    LEN_CHALLENGE = 100\n",
    "    rest_points = challenge_dataset.get_rest()\n",
    "    challenge_dataloader = torch.utils.data.DataLoader(rest_points, batch_size=LEN_TRAINING - (2*LEN_CHALLENGE))\n",
    "    rest_x, rest_y = next(iter(challenge_dataloader))\n",
    "    # Get loss values for these "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the Purchase-100 dataset consisting of 197324 records and 600 attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7ff4d31f3e4437890964ce7af92ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scenario:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c55a07500f40acb0c83e6f59070214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phase:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0807af1de0124e6b817e480b9538de43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7727dad349ef4d76bf5087b7f8afa9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3758689e04d54c0f9b1c2ebcfb4f5213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec6ee92a27b47f59865c3399ab50321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phase:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6248ffb71f4a49c5934e9c0cfaef0865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b211cc2891004fa4aada0b66937fbe6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd62f80119b14f53ab86b2990344c276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88a3f8684d04c6b92f39ac1c531cf75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phase:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9221d06e5304701bd78cf502dddc72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a4a280de824f459f6471b5b131c8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CHALLENGE = \"purchase100\"\n",
    "LEN_TRAINING = 150000\n",
    "LEN_CHALLENGE = 100\n",
    "\n",
    "scenarios = os.listdir(CHALLENGE)\n",
    "phases = ['dev', 'final', 'train']\n",
    "\n",
    "dataset = load_purchase100(dataset_dir=\"/u/as9rw/work/MICO/data\")\n",
    "\n",
    "for scenario in tqdm(scenarios, desc=\"scenario\"):\n",
    "    for phase in tqdm(phases, desc=\"phase\"):\n",
    "        root = os.path.join(CHALLENGE, scenario, phase)\n",
    "        for model_folder in tqdm(sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])), desc=\"model\"):\n",
    "            path = os.path.join(root, model_folder)\n",
    "            challenge_dataset = ChallengeDataset.from_path(path, dataset=dataset, len_training=LEN_TRAINING)\n",
    "            challenge_points = challenge_dataset.get_challenges()\n",
    "            \n",
    "            model = load_model('purchase100', path)\n",
    "            challenge_dataloader = torch.utils.data.DataLoader(challenge_points, batch_size=2*LEN_CHALLENGE)\n",
    "            features, labels = next(iter(challenge_dataloader))\n",
    "\n",
    "            # This is where you plug in your membership inference attack\n",
    "            # Combine preds from both\n",
    "            # Got 0.1106 score\n",
    "            # predictions = neighborhood_and_loss(model, features, 0.01, 10)\n",
    "            # predictions = normalize_preds(predictions)\n",
    "            \n",
    "            # Meta-classifier :Random Forest, directly across all data/models\n",
    "            # Got 0.1121 score\n",
    "            # Scenario-wise meta-clfs got 0.1231 score\n",
    "            # processed_features = neighborhood_and_loss(model, features, labels, as_features=True)\n",
    "            \n",
    "            # Meta-classifier: Random forest, on gradient updates\n",
    "            # Got 0.0716 score\n",
    "            # processed_features = get_gradient_norm(model, features, labels)\n",
    "            \n",
    "            # Meta-classifier: Random forest, on gradient updates, loss, and robustness\n",
    "            # Got 0.1224 score\n",
    "#             processed_features = gradient_and_robustness(model, features, labels)\n",
    "            \n",
    "            # Meta-classifier on multiple gradient updates (RF)\n",
    "            # Got score\n",
    "            processed_features = get_gradient_update_norms(model, features, labels)\n",
    "            \n",
    "            predictions = meta_clfs[scenario].predict_proba(processed_features)[:, 1]\n",
    "\n",
    "            assert np.all((0 <= predictions) & (predictions <= 1))\n",
    "\n",
    "            with open(os.path.join(path, \"prediction.csv\"), \"w\") as f:\n",
    "                 csv.writer(f).writerow(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "\n",
    "Let's see how the attack does on `train`, for which we have the ground truth. \n",
    "When preparing a submission, you can use part of `train` to develop an attack and a held-out part to evaluate your attack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mico_competition.scoring import tpr_at_fpr, score, generate_roc, generate_table\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "FPR_THRESHOLD = 0.1\n",
    "\n",
    "all_scores = {}\n",
    "phases = ['train']\n",
    "\n",
    "for scenario in tqdm(scenarios, desc=\"scenario\"): \n",
    "    all_scores[scenario] = {}    \n",
    "    for phase in tqdm(phases, desc=\"phase\"):\n",
    "        predictions = []\n",
    "        solutions  = []\n",
    "\n",
    "        root = os.path.join(CHALLENGE, scenario, phase)\n",
    "        for model_folder in tqdm(sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])), desc=\"model\"):\n",
    "            path = os.path.join(root, model_folder)\n",
    "            predictions.append(np.loadtxt(os.path.join(path, \"prediction.csv\"), delimiter=\",\"))\n",
    "            solutions.append(np.loadtxt(os.path.join(path, \"solution.csv\"),   delimiter=\",\"))\n",
    "\n",
    "        predictions = np.concatenate(predictions)\n",
    "        solutions = np.concatenate(solutions)\n",
    "        \n",
    "        scores = score(solutions, predictions)\n",
    "        all_scores[scenario][phase] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the ROC curve for the attack and see how the attack performed on different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "for scenario in scenarios:\n",
    "    fpr = all_scores[scenario]['train']['fpr']\n",
    "    tpr = all_scores[scenario]['train']['tpr']\n",
    "    fig = generate_roc(fpr, tpr)\n",
    "    fig.suptitle(f\"{scenario}\", x=-0.1, y=0.5)\n",
    "    fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for scenario in scenarios:\n",
    "    print(scenario)\n",
    "    scores = all_scores[scenario]['train']\n",
    "    scores.pop('fpr', None)\n",
    "    scores.pop('tpr', None)\n",
    "    display(pd.DataFrame([scores]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging the submission\n",
    "\n",
    "Now we can store the predictions into a zip file, which you can submit to CodaLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "phases = ['dev', 'final']\n",
    "experiment_name = \"predictions_gradient_multiupdates\"\n",
    "\n",
    "with zipfile.ZipFile(f\"{experiment_name}.zip\", 'w') as zipf:\n",
    "    for scenario in tqdm(scenarios, desc=\"scenario\"): \n",
    "        for phase in tqdm(phases, desc=\"phase\"):\n",
    "            root = os.path.join(CHALLENGE, scenario, phase)\n",
    "            for model_folder in tqdm(sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])), desc=\"model\"):\n",
    "                path = os.path.join(root, model_folder)\n",
    "                file = os.path.join(path, \"prediction.csv\")\n",
    "                if os.path.exists(file):\n",
    "                    zipf.write(file)\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"`prediction.csv` not found in {path}. You need to provide predictions for all challenges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mico)",
   "language": "python",
   "name": "mico"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c823568a0650a753a55947c22141ec594c2fc02bd68b5a71e505ecc57f17796"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
